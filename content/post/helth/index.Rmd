---
title: "How perceptions of community behavior improve COVID-19 predictions"
summary: "Can crowdsourced perceptions improve pandemic forecasting? In this project, I explore how reports of community compliance with public health measures shaped COVID-19 spreadâ€”and highlight a powerful Bayesian model we used to predict future case trends."
date: "2022-03-20"
image: "/images/metal_ball.png?v=2"
skills:
    - Time-series forecasting
    - Bayesian modeling
    - Multimodal data integration
    - R, Python
read_time: 10
output: bookdown::html_document2
---

```{r setup, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(paletteer)
library(DT)
library(reactable)
library(scales)
knitr::opts_chunk$set(collapse = TRUE, cache=FALSE, out.width="100%")
qual <- paletteer_d('nord::aurora')
sequ <- colorRampPalette(paletteer_d('nord::silver_mine'))(100)
d <- read.csv('post_data/covid_perceptions.csv')
census <- read.csv('post_data/nst-est2019-alldata.csv')

# --- FORMAT DATA --- #
exclude <- c('', 'district of columbia', 'unknown', 'other (please specify)')
colnames(census) <- tolower(colnames(census))
census <- census %>% 
    select(name, popestimate2019) %>% 
    rename(state=name) %>% 
    mutate(state = tolower(state))
d <- d %>% 
    mutate(id = 1:nrow(.)) %>% 
    relocate(id, .before = start.date) %>% 
    select(-respondent.id) %>% 
    gather(question, response, q1:q44) %>% 
    filter(question %in% paste0('q', 1:21), 
           !state %in% exclude) %>% 
    filter(!is.na(response)) %>% 
    mutate(question = as.integer(str_replace(question, 'q', '')),
           response = as.integer(response)) 
uspop <- sum(census[census$state %in% unique(d$state),]$popestimate2019)
census <- census %>% 
    mutate(pop_prop = census$popestimate2019 / uspop) %>% 
    filter(state %in% unique(d$state)) 

# Make observed state proportions
N <- length(unique(d$id))
census <- d %>% 
    group_by(state, id) %>% 
    summarize(n()) %>% 
    group_by(state) %>% 
    summarize(observed = n()) %>% 
    mutate(obs_prop = observed / N) %>% 
    inner_join(census)
    
```


{{% highlightbox %}}
ðŸ“„ <a href="http://dx.doi.org/10.2196/39336" target="_blank">Read the Paper</a>  
ðŸ–¼ <a href="/portfolio/Braun_MIDAS_2022.pdf" target="_blank">View the Poster</a>  
ðŸ’¾ <a href="https://osf.io/9khrq" target="_blank">Download the data</a>

### TL;DR 

I built models that accurately predicted COVID-19 case numbers up to three weeks aheadâ€”using not just case data, but also how well people thought their communities were following safety guidelines like social distancing. This approach shows how public perception can be used to improve real-time forecasting tools for organizations like the CDC.


### Key Skills
* ðŸ“ˆ **Time-Series Forecasting** â€“ Applied epidemiological models to predict COVID-19
cases using behavioral and clinical data

* ðŸ§  **Bayesian Modeling** â€“ Leveraged probabilistic forecasts to support real-time
public health decision-making

* ðŸ”— **Multimodal Data Integration** â€“ Combined self-reported survey data with case count time series to enhance model accuracy

* ðŸ“Š **Communicating Uncertainty** â€“ Created clear, decision-relevant visualizations for probabilistic outcomes and behavioral drivers

### What I Learned

How to turn crowdsourced human behavior data into probabilistic forecasts using Bayesian modelingâ€”providing community leaders and health systems with more informative, uncertainty-aware predictions of epidemic spread.

{{% /highlightbox %}}


## <u>What we were interested in </u>

### What if we could improve pandemic forecasting by asking people how well their neighbors are following the rules?

Modeling the path of infectious diseases is a popular and important area of
research,[^model1]<sup>,</sup>[^model2] hugely supporting public health
situational awareness during a pandemic.[^important1]<sup>,</sup>[^important2]
Most models of infectious disease are purely *computational*, meaning they rely
only on existing data found on the internet to support their predictions.
[^computational1]<sup>,</sup>[^computational2] However, including data generated
by humans has been proven to enhance forecasting infectious
diseases.[^human1]<sup>,</sup>[^human2]<sup>,</sup>[^human3]<sup>,</sup>[^human4]

[^model1]:  Chelsea S Lutz, Mimi P Huynh, Monica Schroeder, Sophia Anyatonwu, F Scott Dahlgren, Gregory Danyluk, Danielle Fernandez, Sharon K Greene, Nodar Kipshidze, Leann Liu, et al. Applying infectious disease forecasting to public health: a path forward using influenza forecasting examples. BMC Public Health, 19(1):1â€“12, 2019.

[^model2]:  Simon Pollett, Michael A Johansson, Nicholas G Reich, David Brett-Major, Sara Y Del Valle, Srinivasan Venkatramanan, Rachel Lowe, Travis Porco, Irina Maljkovic Berry, Alina Deshpande, et al. Recommended reporting items for epidemic forecasting and prediction research: The epiforge 2020 guidelines. PLoS medicine, 18(10):e1003793, 2021.

[^computational1]: Sara Y Del Valle, Benjamin H McMahon, Jason Asher, Richard Hatchett, Joceline C Lega, Heidi E Brown, Mark E Leany, Yannis Pantazis, David J Roberts, Sean Moore, et al. Summary results of the 2014-2015 darpa chikungunya challenge. BMC infectious diseases, 18(1):1â€“14, 2018.

[^computational2]:  Michelle V Evans, Tad A Dallas, Barbara A Han, Courtney C Murdock, and John M Drake. Data-driven identification of potential zika virus vectors. elife, 6:e22053, 2017.

[^important1]: Matthew Biggerstaff, Rachel B Slayton, Michael A Johansson, and Jay C Butler. Improving pandemic response: Employing mathematical modeling to confront coronavirus disease 2019. Clinical Infectious Diseases, 2021.

[^important2]:  Estee Y Cramer, Evan L Ray, Velma K Lopez, Johannes Bracher, Andrea Brennen, Alvaro J Castro Rivadeneira, Aaron Gerding, Tilmann Gneiting, Katie H House, Yuxin Huang, et al. Evaluation of individual and ensemble probabilistic forecasts of covid-19 mortality in the us. Medrxiv, 2021.

[^human1]: Nikos I Bosse, Sam Abbott, Johannes Bracher, Habakuk Hain, Billy J Quilty, Mark Jit, Edwin van Leeuwen, Anne Cori, Sebastian Funk, et al. Comparing human and model-based forecasts of covid-19 in germany and poland. medRxiv, 2021.

[^human2]: David C Farrow, Logan C Brooks, Sangwon Hyun, Ryan J Tibshirani, Donald S Burke, and Roni Rosenfeld. A human judgment approach to epidemiological forecasting. PLoS computational biology, 13(3):e1005248, 2017.

[^human3]: Thomas McAndrew and Nicholas G Reich. An expert judgment model to predict early stages of the covid-19 outbreak in the united states. Medrxiv, 2020.

[^human4]: Gabriel Recchia, Alexandra LJ Freeman, and David Spiegelhalter. How well did experts and laypeople forecast the size of the covid-19 pandemic? PloS one, 16(5):e0250935, 2021.

> <p style="font-size: 1.5em;">Can we identify the most effective non-pharmaceutical interventions by seeing which ones most improve a model's ability to predict future cases?</p>

During the COVID-19 pandemic, the CDC issued many *non-pharmaceutical
interventions* (NPIs), or behavior-based ways of mitigating the spread of the
disease (eg, social distancing). During the pandemic, I'm sure we all had some
intuitions around whether people were adhering to these NPIs, how that adherence
changed throughout the course of the pandemic, and how effective that NPI might
have been for reducing the spread of the disease. In this study,
we wanted to know whether the degree to which people were adhering to NPIs could
improve predictions of infectious disease spread, and see which NPIs improved
predictions the most.



```{r fig.cap="Can we better predict the spread of disease if we can measure how many people are wearing masks or social distancing?", echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("masked_crowd.jpg")
```

## <u> How we did it </u>
### We asked a crowd 21 questions about their community's compliance with NPIs over 35 weeks and tested whether their responses improved COVID-19 case forecasts

Each week during the COVID-19 pandemic---from August 2020 through April
2021---we sent surveys to people across the US asking them 21 core questions
about how well their communities were complying with the CDC's NPI regulations
(see *Figure \@ref(fig: survey)*). 

```{r survey, echo=FALSE, fig.cap="The 21 survey questions posed to the crowd. Responses were on a Likert scale from 0% to 100% in increments of 20. Participants also had the option of selecting 'Don't know'."}
qs <- read.csv('post_data/question_coding.csv')
colnames(qs) <- c('drop', 'drop', 'Preface', 'Question')

reactable(
  qs[,c('Preface', 'Question')],
  searchable = TRUE,
  sortable = FALSE,
  highlight = TRUE,
  pagination = TRUE,
  paginationType = "numbers",         # horizontal page numbers (default)
  showPageSizeOptions = FALSE,        # hide dropdown
  defaultPageSize = 10,
  theme = reactableTheme(
    backgroundColor = "transparent",  # let it inherit from the page
    color = "inherit",
    borderColor = "inherit",
    highlightColor = "rgba(0, 0, 0, 0.05)",
    stripedColor = "rgba(0, 0, 0, 0.02)",
    style = list(fontSize = "0.95em"),
    pageButtonStyle = list(
      background = "transparent",
      color = "inherit",
      border = "1px solid #ccc",
      borderRadius = "4px",
      padding = "4px 8px",
      margin = "0 2px"
    ),
    pageButtonHoverStyle = list(
      background = "#ddd"
    ),
    pageButtonActiveStyle = list(
      background = "#bbb",
      fontWeight = "bold"
    )
  )
)

```


We collected a total of 10,852 survey responses across three different survey
platforms. These responses were evenly distributed over time and roughly
geographically representative of the US population (see *Figure
\@ref(fig:responses)*).

```{r responses, fig.cap=""}

dates <- unique(d$year_month_day)
dates <- dates[order(unique(d$mw))]
dates <- ifelse((seq_along(dates)+3) %% 4 == 0, dates, "")

d %>% 
    group_by(mw, survey_platform) %>% 
    summarize(year_month_day = unique(year_month_day),
              count = length(unique(id))) %>% 
    mutate(survey_platform = recode(survey_platform, `sm_volunteer` = 'Platform A',
                                    `sm_paid` = 'Platform B', `pollfish` = 'Platform C')) %>% 
    ggplot(aes(x = reorder(year_month_day, mw), y = count, 
               color=survey_platform, group=survey_platform)) + 
    geom_line() + 
    geom_point() +
    labs(
        x = 'Date',
        y = 'Number of responses',
        color = 'Survey platform'
    ) + 
    ylim(0, 450) +
    scale_x_discrete(labels = dates, breaks = dates) +
    scale_color_manual(values = qual[c(1, 4, 5)]) + 
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = c(.8, .8))


census %>% 
    mutate(state_title = str_to_title(state),
           expected = pop_prop * N) %>% 
    relocate(expected, .after = observed) %>% 
    mutate(diff = abs(observed - expected)) %>% 
    mutate(outlier = ifelse(diff > quantile(diff, probs = .9), 'yes', 'no')) %>% 
    mutate(outlier_label = ifelse(outlier == 'yes', state_title, NA)) %>% 
    ggplot(aes(x = expected, y = observed)) + 
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed') + 
    geom_point(aes(color = outlier)) + 
    ggrepel::geom_label_repel(aes(label = outlier_label)) +
    ylim(0, 1200) + 
    xlim(0, 1200)

```































