<!DOCTYPE html>
<html><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap-grid.min.css" rel="stylesheet">
    <link href="/images/fav/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
    <script src="/js/code-folding.js?v=1.0.10" defer></script>
    <link href="/images/fav/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
    <link href="/images/fav/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
    <link href="/images/fav/site.webmanifest" rel="manifest"/>
    <title>Publications | David Braun</title>
    <link href="/css/styles.css?v=1" rel="stylesheet"/>
    <link href="/css/override.css?v=10" rel="stylesheet"/>
    
</head><body><header>
    <div class="logo h-card">
        <a class="u-url" href="//localhost:4321/">
            <img alt="Site Logo" class="site-logo u-photo" src="/images/logo.png"/>
        </a>
    </div>
    <nav class="menu">
        <ul>
                <li>
                    <a href="/about/">About</a>
                </li>
                <li>
                    <a href="/#contact">Contact</a>
                </li>
                <li>
                    <a href="/#results">Projects</a>
                </li>
                <li>
                    <a href="/publications/">Publications</a>
                </li>
        </ul>
    </nav>
</header>

    

    <section class="hero"
    style='background-image: url("/images/7.jpg"); background-size: cover; background-position: center; background-repeat: no-repeat; width: 100%;'
    id="heroSection" onload="adjustTextColor(this)">
    </section>
    <main class="post h-entry container row">
        <div class="post">
            <div class="post-data row">
                <div>
                    <h1 class="p-name">Publications</h1>
                </div>
                
                <span rel="author" class="p-author h-card">
                    <img class="u-photo hidden" src="" />
                    <span class="p-name hidden" rel="me"></span>
                </span>
            </div>
            <div class="e-content">
                <h3 id="under-review">Under review</h3>
<ul>
<li><strong>Braun, D.</strong>, Shareef-Trudeau, L., Rao, S., Cheesebrough, C., Kam, J. W. Y., Kucyi, A.
(<em>Under review</em>). State anxiety modulates the relationship between neural processing
of heartbeats and spontaneous fluctuations in subjective arousal. <em>Journal of Neuroscience</em>.
<a href="https://doi.org/10.1101/2025.03.26.645574">https://doi.org/10.1101/2025.03.26.645574</a></li>
</ul>
<h3 id="2024">2024</h3>
<ul>
<li>
<p>McAndrew, T., Gibson, G. C., <strong>Braun, D.</strong>, Srivastava, A, &amp; Brown, K. (2024). Chimeric forecasting: An
experiment to leverage human judgment to improve forecasts of infectious disease using simulated
surveillance data. <em>Epidemics, 47</em>, 100756. <a href="https://doi.org/10.1016/j.epidem.2024.100756">https://doi.org/10.1016/j.epidem.2024.100756</a></p>
</li>
<li>
<p>Bounyarith, T., <strong>Braun, D.</strong>, &amp; Kucyi, A. (2024). Examining the neural bases of spontaneous mental
experiences with real-time fMRI. <em>Peer Community in Registered Reports</em> [Stage 1 Registered Report: In-
Principle Accepted]. <a href="https://osf.io/sd4hu">https://osf.io/sd4hu</a></p>
</li>
<li>
<p>Kucyi, A., Anderson, N., Bounyarith, T., <strong>Braun, D.</strong>, Shareef-Trudeau, L., Treves, I., &hellip; &amp; Hung, S. (2024).
Individual variability in neural representations of mind wandering. <em>Network Neuroscience</em>, 8, 808-856.
<a href="https://doi.org/10.1162/netn_a_00387">https://doi.org/10.1162/netn_a_00387</a>.</p>
</li>
<li>
<p>McAndrew, T., Gibson, G. C., <strong>Braun, D.</strong>, Srivastava, A. &amp; Brown, K. (2024). Chimeric Forecasting: An
experiment to leverage human judgment to improve forecasts of infectious disease using simulated
surveillance data. <em>Epidemics</em>, 47, 100756. <a href="https://doi.org/10.1016/j.epidem.2024.100756">https://doi.org/10.1016/j.epidem.2024.100756</a>.</p>
</li>
<li>
<p>Mittelstadt, V., Mackenzie, I. G., <strong>Braun, D.</strong>, &amp; Arrington, K. (2024). Reactive and proactive control
processes in voluntary task choice. <em>Memory and Cognition</em>, 52, 419-429.
<a href="https://doi.org/10.3758/s13421-023-01470-y">https://doi.org/10.3758/s13421-023-01470-y</a></p>
</li>
</ul>
<h3 id="2022">2022</h3>
<ul>
<li>
<p>McAndrew, T., Codi, A., Cambeiro, J., Besiroglu, T., <strong>Braun, D.</strong>, Chen, E., Enrique Urtubey de CÃ©saris, L.,
Luk, D. (2022). Chimeric forecasting: Combining probabilistic predictions from computational models and
human judgment. <em>BMC Infectious Diseases</em>, 22, 1-17. <a href="https://doi.org/10.1186/s12879-022-07794-5">https://doi.org/10.1186/s12879-022-07794-5</a>.</p>
</li>
<li>
<p><strong>Braun, D.</strong>, Ingram, D., Ingram, D., Khan, B., Marsh, J., &amp; McAndrew, T. (2022). Crowdsourced
perceptions and COVID-19: Improving computational forecasts of US national incident cases of COVID-19
with crowdsourced perceptions of human behavior. <em>JMIR Public Health Surveill</em>, 8, 1-18.
<a href="http://dx.doi.org/10.2196/39336">http://dx.doi.org/10.2196/39336</a></p>
</li>
<li>
<p>Codi, A., Luk, D., <strong>Braun, D.</strong>, Cambeiro, J., Besiroglu, T. Chen, E., Enrique Urtubey de Cesaris, L.,
Bocchini, P., McAndrew, T. (2022). Aggregating human judgment probabilistic predictions of COVID-19
transmission, burden, and preventative measures. <em>American Journal of Public Health.</em>
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9016644/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9016644/</a></p>
</li>
<li>
<p>McAndrew, T., Majumder, M. S., Lover, A. A., Venkatramanan, S., Boccini, P., Besiroglu, T., Codi, A.,
**Braun, D.*, Dempsey, G., Abbott, S., Chevalier, S., Bosse, N. I., Cambeiro, J. (2022). Human judgment
forecasts of human monkeypox transmission and burden in non-endemic countries. <em>Lancet Digital Health</em>.
<a href="https://doi.org/10.1016/S2589-7500(22)00127-3">https://doi.org/10.1016/S2589-7500(22)00127-3</a></p>
</li>
</ul>
<h3 id="earlier">Earlier</h3>
<ul>
<li>
<p>Marvel, C. J., Bates, J. E., Hambric, C. E., <strong>Braun, D.</strong>, Arrington, C. M., &amp; Harmer, M. P. (2021). The
Lehigh Presidential Nano-Human Interface Initiative: Covergence of materials and cognitive sciences.
<em>MRS Bulletin.</em> <a href="https://doi.org/10.1557/s43577-021-00232-y">https://doi.org/10.1557/s43577-021-00232-y</a></p>
</li>
<li>
<p><strong>Braun, D.</strong>, Arrington, C. M. (2018). Assessing the role of effort and reward in task selection using a
reward-based voluntary task switching paradigm. <em>Psychological Research</em>, 82, 54-64.
<a href="https://doi.org/10.1007/s00426-017-0919-x">https://doi.org/10.1007/s00426-017-0919-x</a></p>
</li>
<li>
<p>Fleck, J. I. &amp; <strong>Braun, D.</strong> (2015). The impact of eye movements on a verbal creativity task. Journal of
<em>Cognitive Psychology</em>, 27, 866-881. <a href="https://doi.org/10.1080/20445911.2015.1036057">https://doi.org/10.1080/20445911.2015.1036057</a></p>
</li>
</ul>

                
            </div>
        </div>
        
    </main>
    <script>
        function adjustTextColor(sectionId) {
            const section = document.getElementById(sectionId);
            const style = window.getComputedStyle(section);
            const imageUrl = style
                .backgroundImage
                .slice(4, -1)
                .replace(/["']/g, "");
            const image = new Image();
            image.src = imageUrl;
            image.onload = function () {
                const canvas = document.createElement('canvas');
                const context = canvas.getContext('2d');
                canvas.width = image.width;
                canvas.height = image.height;
                context.drawImage(image, 0, 0, image.width, image.height);
                const imageData = context.getImageData(0, 0, canvas.width, canvas.height);
                const data = imageData.data;
                let r = 0,
                    g = 0,
                    b = 0;
                for (let i = 0; i < data.length; i += 4) {
                    r += data[i];
                    g += data[i + 1];
                    b += data[i + 2];
                }
                r /= (data.length / 4);
                g /= (data.length / 4);
                b /= (data.length / 4);
                const brightness = Math.sqrt(0.299 * r * r + 0.587 * g * g + 0.114 * b * b);
                const textColor = brightness > 128
                    ? 'black'
                    : 'white';
                const shadowColor = brightness > 128
                    ? 'rgba(255, 255, 255, 0.7)'
                    : 'rgba(0, 0, 0, 0.7)';
                const textOverlay = document.getElementById('textOverlay');
                textOverlay.style.color = textColor;
                textOverlay.style.textShadow = `0 0 5px ${shadowColor}, 0 0 10px ${shadowColor}, 0 0 20px ${shadowColor}, 0 0 40px ${shadowColor}, 0 0 80px ${shadowColor}, 0 0 90px ${shadowColor}, 0 0 100px ${shadowColor}, 0 0 150px ${shadowColor}`;
            };
        }
        
        adjustTextColor('heroSection');
    </script>
    <script>
        window.addEventListener('DOMContentLoaded', () => {
            const observerForTableOfContentActiveState = new IntersectionObserver(entries => {
                entries.forEach(entry => {
                    const id = entry
                        .target
                        .getAttribute('id');

                    if (entry.intersectionRatio > 0) {
                        clearActiveStatesInTableOfContents();
                        document
                            .querySelector(`nav li a[href="#${id}"]`)
                            .parentElement
                            .classList
                            .add('active');
                    }
                });
            });
            document
                .querySelectorAll('h1[id],h2[id],h3[id],h4[id]')
                .forEach((section) => {
                    observerForTableOfContentActiveState.observe(section);
                });

        });

        function clearActiveStatesInTableOfContents() {
            document
                .querySelectorAll('nav li')
                .forEach((section) => {
                    section
                        .classList
                        .remove('active');
                });
        }
    </script>
<footer>
    <div class="social-media">
        
     
        
    </div>
    <div class="logo h-card">
        <a class="u-url" href="//localhost:4321/">
            <img alt="Site Logo" class="site-logo u-photo" src="/images/logo.png"/>
        </a>
        <p>Â© 2025
            David Braun</p>
    </div>
    <nav>
        <ul>
                <li>
                    <a href="/about/">About</a>
                </li>
                <li>
                    <a href="/#contact">Contact</a>
                </li>
                <li>
                    <a href="/#results">Projects</a>
                </li>
                <li>
                    <a href="/publications/">Publications</a>
                </li>
        </ul>
    </nav>
    <script>
    window.store = {
    
    
    "\/\/localhost:4321\/": {
        
        "title": "David Braun",
            "tags": [],
    "content": "", 
    "url": "\/\/localhost:4321\/"
        },
    
    
    "\/\/localhost:4321\/2025\/04\/24\/does-the-brain-cope-with-anxiety-by-numbing-out\/": {
        
        "title": "Does the brain cope with anxiety by numbing out?",
            "tags": [],
    "content": " ð Read the Paper\nð¼ View the Poster\nTL;DR I used EEG and unsupervised machine learning techniques to analyze how the brain processes internal signals (interoception) and its link to anxiety. I found that the brain is less sensitive to internal signals during high energy states, a pattern that was amplified during states of anxiety\u0026mdash;insights from this project are relevant to stress detection and human-centered design.\nKey Skills ð§¹ Data Wrangling â Cleaned high-dimensional EEG \u0026amp; physiological data ð Exploratory Data Analysis â Examined relationships between physiological and behavioral measures ð¤ Machine learning â Applied signal separation for artifact removal and clustering to extract insights ð Visualization â Created clear, insight-driven plots What I Learned How to extract meaningful signals from noisy biological data, a skill transferable to real-world time-series analysis (wearables, health tech, user behavior modeling).\nSometimes life is angsty. We\u0026rsquo;ve all been there\u0026mdash;you\u0026rsquo;re about to talk in front of a group, have a difficult conversation with someone, or do some task where a mistake would be really bad. While it might be healthier not to avoid anxiety, sometimes it\u0026rsquo;s just easier to reach for a distraction. What you may not know is that your brain might subconsciously be trying to avoid anxiety as well.\nFigure 1: Distraction is an easy way to avoid feeling anxiety.\n## [1] \u0026#34;/home/dave/Dropbox/post_doc/professional/career/davebraundotnet/blogdown/content/post/hep\u0026#34; What we were interested in We wanted to look at how the brain responds to heartbeats across different states of anxiety and energy Previous research suggests that anxiety disrupts our brain\u0026rsquo;s subconsious sensitivity to bodily signals, a sensitivity referred to as interoception. 1 Interoception is our brain\u0026rsquo;s way of sensing and regulating many of the body\u0026rsquo;s functions, such as breathing and responding to heart rhythms.\nDifferences in interoception have also been linked to differences in one\u0026rsquo;s sense of energy or activation2,3\u0026mdash;a sense we\u0026rsquo;ll refer to as subjective arousal. Subjective arousal\u0026mdash;much like anxiety and interoception\u0026mdash;is continually and spontaneously fluctuating, even when people are just at rest. But no previous study has looked at how subjective arousal, anxiety, and interoception spontaneously fluctuate with each other at rest.\nWe wanted to look at how the brain responds to bodily signals\u0026mdash;specifically, heartbeats\u0026mdash;during spontaneous fluctuations in subjective arousal when people are at rest, and how this interaction might be different for those low vs. high in anxiety. How these processes play out at rest might be important because certain brain networks are typically only active at rest, and these networks contribute to patterns like rumination and worry that feed into anxiety. 4\nUnderstanding how the brain shifts interoception in response to changing subjective arousal states during rest might help us better understand processes that underlie anxiety.\nHow we did it We measured electrical activity from the brain and heart during rest and occassionally asked people how they were feeling A precise way to measure neural interoception is through what\u0026rsquo;s called the heartbeat evoked potential (HEP), which is basically the brain\u0026rsquo;s electrophysiological response to heartbeats.5 Your brain is constantly emitting electricity (emanating from the communication between neurons), and when the brain\u0026rsquo;s electrical response is high following a heartbeat, that\u0026rsquo;s thought to reflect higher levels of neural interoceptive sensitivity to the heart.\nParticipants (51) came into our lab and completed a survey indicating their level of state anxiety, or how much anxiety they were experiencing in that moment. We then fitted them with an EEG cap to measure their neural electrical signals. This EEG cap included 31 scalp electrodes plus an electrode that we placed on participants\u0026rsquo; back (ECG) to measure their heartbeat activity (see Figure 2).\nFigure 2: (A) Cartoonized illustration of EEG cap and (B) placement of EEG electrodes in the experiment according to the 10-20 coordinate system\nParticipants stared at a blank screen and were told to let their minds wander freely. Every so often, a set of questions (ie, a thought probe) appeared asking participants to report different aspects of their experience. Most important was the question asking about their level of subjective arousal, which includes feelings of energy linked to emotion, and participants gave ratings on a scale from 0 (completely deactivated) to 100 (completely activated; see Figure 3A).\nFigure 3: Trial sequence and analysis approach. (A) Participants were instructed to let their minds wander freely while viewing a blank screen and occasionally reported on aspects of their experience (such as their level of subjective arousal). (B) Brain EEG signals before a thought probe and directly after heartbeats were averaged together, separated by a median split in subjective arousal, and submitted to a whole-brain analysis (ie, cluster-based permutation analysis).\nTo calculate the HEP, I isolated the 10 s period before the onset of a thought probe and extracted the EEG signals (from all 31 electrodes) that occurred directly after a heartbeat (see Figure 3B). I then separated the 10 s pre-probe periods according to whether the subjective arousal rating obtained during the thought probe was high or low (based on a within-participants median split on subjective arousal).\nFinally, I averaged across the high vs. low levels of subjective arousal (collapsing across heartbeats and probes), which gave me one averaged brain signal value for timepoints immediately following a heartbeat, for each EEG channel, for each subjective arousal condition, across all participants. In other words, we had for each participant an array of shape (timepoints x channel x condition), or (100 x 31 x 2).\nUltimately, we wanted to know how interoceptive sensitivity to cardiac signals (as measured with HEPs) is different in periods leading up to self-reported high vs. low states of subjective arousal. To assess this, I took those arrays defined in the previous paragraph and conducted a whole brain analysis using what\u0026rsquo;s called a cluster-based permutation test.\nCluster-based permutation test is a super powerful and flexible data-driven method for finding differences between conditions in high-dimensional space. In our example, the conditions are a median split on subjective arousal ratings, each thought probe was labelled as \u0026ldquo;high\u0026rdquo; or \u0026ldquo;low\u0026rdquo; subjective arousal. The high dimensional space is the remaining (time x channels) array of EEG data for each participant. In short, we want to find whether EEG voltage is different across high vs. low levels of subjective arousal conditions at any timepoint(s) and for any EEG channel(s).\nExpand this section to optionally read more about cluster-based permutation test and see how I implemented it with code. Technical spotlight: Cluster-based permutation testing What is a cluster-based permutation test? We\u0026rsquo;re trying to find a meaningful pattern in brain data, but brain data is massive. In our case, we\u0026rsquo;re looking across 6,200 data points. If we tested each one individually, we\u0026rsquo;d almost certainly get false positives (random noise that looks meaningful).\nA cluster-based permutation test helps solve this.\nThe core idea:\nInstead of asking \u0026ldquo;is this single time point significant?\u0026rdquo;, we ask:\nAre there clusters of nearby data points that are all showing an effect?\nIf a real difference exists, we expect it to show up not just as a blip, but as a spatially or temporally coherent cluster.\nHow it works:\nCompute the actual data difference between two groups (eg, high vs. low subjective arousal) at every point in time or space. Identify clusters of adjacent points that pass some basic threshold (eg, t-values \u0026gt; 2). Calculate a cluster-level statistic (like the sum of t-values) for each cluster. Randomly shuffle the group labels many times (eg, 5,000 permutations) and repeat steps 1-3 each time. Compare your original clusters to the shuffled ones. If a real cluster is bigger than 95% of the ones you\u0026rsquo;d see by chance, it\u0026rsquo;s considered statistically significant. What makes this approch so powerful is that it doesn\u0026rsquo;t rely on strict assumptions about what the data would look like if there were no effect (ie, a null distribution). Instead, it builds a null distribution directly from the data. This allows us to compare many data points over an arbitrary number of dimensions to look for significant effects, making cluster-based permutation testing the perfect tool for finding differences in our brain data.\nHow I implemented it To make the cluster-based permutation test flexible and reproducible, I wrote a collection of functions in both Python and R and coordinated them with R\u0026rsquo;s fancy reticulate package. The Python functions actually ran the cluster test using mne.stats.spatio_temporal_cluster_test, and the R functions were mostly for summarizing the result. I\u0026rsquo;m highlighting here the main Python function doing most of the work, but you can see the full set of functions on GitHub (for Python and R).\nThe whole main Python function:\nfrom pathlib import Path import pickle from mne.stats import spatio_temporal_cluster_test def permutation_cluster_test(item, low_anchor, bads, time_window_min=0.25, initial_alpha=0.01, path=Path(\u0026#39;analysis/data/derivatives/hep/06-evoked-clean\u0026#39;)): \u0026#39;\u0026#39;\u0026#39; Conducts a permutation-based clustering analysis across a median split of item, analyzing time points from time_window_min to end of epoch. --- PARAMETERS --- ------------------ item (str): Item name low_anchor (str): Name of low anchor on the scale (all lowercase) time_window_min (float): Analyze time points after this value initial_alpha (float): Alpha for finding initial clusters path (pathlib.Path): Path to directory containing data --- RETURNS --- --------------- out (dict) containing results of permutation test: t_obs: (N_timepoints x M_channels) matrix with t values as elements clusters: list of (array(time_idx, ...), array(channel_idx, ...)) tuples of all found clusters p_values: np.array of shape (N_clusters,) where each element is a p value \u0026#39;\u0026#39;\u0026#39; # Open dictionary assert(isinstance(item, str)) file = \u0026#39;eeg_dict_{}.pkl\u0026#39;.format(item) full_path = path / Path(file) with open(full_path, \u0026#39;rb\u0026#39;) as f: dic = pickle.load(f) # Ensure bads is a list of ints if isinstance(bads, list): bads = [int(x) for x in bads] else: bads = [int(bads)] # Get numpy arrays of shape (subjects, time, chans) for each condition low, high = _format_for_clustering(dic, low_anchor, bads) # Get a sample evoked object for computing distances probe_set = dic[list(dic.keys())[0]][low_anchor] sample_evoked = probe_set[list(probe_set.keys())[0]] adjacency, _ = find_ch_adjacency(sample_evoked.info, \u0026#39;eeg\u0026#39;) # Get the first index of timepoint thats \u0026gt;= the min timepoint time_window_idx = [i for i, e in enumerate(sample_evoked.times) if e \u0026gt;= time_window_min][0] times = sample_evoked.times[time_window_idx:] channels = sample_evoked.info[\u0026#39;ch_names\u0026#39;] # Format data as list of arrays X = [low[:, time_window_idx:, :], high[:, time_window_idx:, :]] # Configure parameters df = low.shape[0] - 1 t_crit = stats.t.ppf(1 - initial_alpha, df) tail = 0 # Run test t_obs, clusters, p_values, _ = spatio_temporal_cluster_test( X, n_permutations=1000, threshold=t_crit, tail=tail, n_jobs=None, seed = 1510, buffer_size=None, adjacency=adjacency, stat_fun=_my_t ) out = {\u0026#39;t_obs\u0026#39;: t_obs, \u0026#39;clusters\u0026#39;: clusters, \u0026#39;p_values\u0026#39;: p_values, \u0026#39;times\u0026#39;: times, \u0026#39;channels\u0026#39;: channels} return out Let\u0026rsquo;s zoom into the important stuff.\nThe actual cluster-based permutation test is being run with this code:\n# Run test t_obs, clusters, p_values, _ = spatio_temporal_cluster_test( X, n_permutations=1000, threshold=t_crit, tail=tail, n_jobs=None, seed = 1510, buffer_size=None, adjacency=adjacency, stat_fun=_my_t ) This function (spatio_temporal_cluster_test) comes from the MNE library, which is an excellent set of EEG/MEG analysis tools written for Python. Let\u0026rsquo;s step through the important input arguments.\nThe input data\nThe input data (X) needs to be formatted as a list of arrays, where each array in the list is data from one of the conditions\u0026mdash;in our case, the two subjective arousal conditions (high vs. low). This formatting is done by the following lines of code:\n# Get numpy arrays of shape (subjects, time, chans) for each condition low, high = _format_for_clustering(dic, low_anchor, bads) # Format data as list of arrays X = [low[:, time_window_idx:, :], high[:, time_window_idx:, :]] I\u0026rsquo;m first calling one of my other functions (not shown above) to format the relevant data into two arrays split by subjective arousal condition. These arrays represent the data across each timepoint and channel for each subject. Next, I\u0026rsquo;m concatenating these two arrays into a list\u0026mdash;taking only a subset of the time window with time_window_idx\u0026mdash;which serves as our final input data X. We\u0026rsquo;re analyzing only a subset of the time window because we want to conduct our statistics only on those measurements that occur at least 250 ms after each heartbeat, which is to ensure that heartbeat activity doesn\u0026rsquo;t contaminate the neural activity we\u0026rsquo;re interested in analyzing.\nThe statistical hyperparameters\nWe need to configure a number of statistical hyperparameters that will influence how the cluster test algorithm runs. We first need to decide on what is a significant threshold for forming a cluster in the first place (ie, the threshold input argument). Since we\u0026rsquo;re only comparing two conditions within participants, the statistical test we use to compute this threshold is a paired-samples t test (which is defined in the _my_t function and passed to the stat_fun input argument of the cluster analysis).\nFor each of the many, many comparisons across data points, the algorithm computes one t test, and the threshold we set determines whether the resulting t value is considered significant. Significant t values that are adjacent to one another in data space are considered to be a \u0026ldquo;cluster\u0026rdquo;. For this type of test, it\u0026rsquo;s common to use an alpha value (ie, false positive rate) of 0.01 as a cluster forming threshold. But we need to convert that alpha to a t value, and for that we use the quantile function of the t distribution (ie, the function that maps probability values to t values, or the inverse cumulative distribution function):\n# Configure parameters df = low.shape[0] - 1 t_crit = stats.t.ppf(1 - initial_alpha, df) tail = 0 The degrees of freedom (df) is the number of participants minus 1, and we subtract alpha from 1 to calculate the cumulative density between the tail of the t distribution and the alpha value. Finally, tail=0 tells the algorithm we want to perform a two-tailed test, meaning either condition can have greater EEG voltage than the other.\nThe algorithm returns the data points of all the clusters found, along with the t values across all data points, and cluster-specific p values. If a cluster has a cluster-level p value of less than .05, we know there was a significant difference across subjective arousal conditions during the cluster\u0026rsquo;s time period and for that cluster\u0026rsquo;s channels. Below (Figure 4) is a heat map of the t values across all data points (with a preview of a signifcant cluster):\n# Read in data m \u0026lt;- readRDS(\u0026#39;post_data/cluster_result.rds\u0026#39;) # Parse time and channels, make data frame channels \u0026lt;- colnames(m$eeg)[which(colnames(m$eeg)==\u0026#39;Fp1\u0026#39;):ncol(m$eeg)] times \u0026lt;- unique(m$eeg$time) times \u0026lt;- times[times \u0026gt;= .25] ts \u0026lt;- as.data.frame(m$result$t_obs) colnames(ts) \u0026lt;- channels ts \u0026lt;- cbind(data.frame(time = times, ts)) # Configure colors and plot blues \u0026lt;- brewer.pal(5, \u0026#39;Blues\u0026#39;) blues_g \u0026lt;- colorRampPalette(c(blues[1], blues[5]))(100) ts %\u0026gt;% gather(channel, t, Fp1:POz) %\u0026gt;% mutate(channel = factor(channel, levels = rev(channels))) %\u0026gt;% ggplot(aes(x = time, y = channel)) + geom_tile(aes(fill = t)) + geom_rect(aes(ymin = \u0026#39;C3\u0026#39;, ymax = 31.5, xmin = .31, xmax = .38), fill = NA, color = \u0026#39;green\u0026#39;, linetype = \u0026#39;dashed\u0026#39;, linewidth = 1.5) + scale_fill_gradientn(colors = blues_g) + labs( x = \u0026#39;Time post heartbeat (s)\u0026#39;, y = \u0026#39;EEG Channels\u0026#39;, fill = \u0026#39;t value\u0026#39; ) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks = element_blank()) Figure 4: Heat map of t values from cluster-based permutation analysis across time and EEG channels. Time is relative to heartbeat onset. First letter of channel suggests it's position on the scalp (ie, F=frontal, C=central, T=temporal, P=parietal, O=occipital). Darker blues indicate higher t values, revealing a larger difference in the EEG signal between subjective arousal conditions. Green dashed lines highlight the significant cluster.\nThat\u0026rsquo;s it for the technical spotlight! Now back to the main post.\nResearch questions In this post, I address three questions with the analyses:\nHow was our measure of subjective arousal related to physiological arousal? How does interoception (as measured with HEP) vary across states of subjective arousal? How does the relationship between interoception and subjective arousal change depending on state anxiety? What we found 1. Subjective arousal was unrelated to physiological arousal and somewhat related to other dimensions of experience To first get a better sense of what we were really measuring with our subjective arousal scale, I looked at how subjective arousal ratings compared to (i) more traditional measures of objective, physiological arousal (such as heartrate), and (ii) how subjective arousal related to other dimensions of ongoing experience that we measured with our thought probes (such as tendency to think about the future). Figure 5 shows participant-specific correlations between subjective arousal and both heartrate and future thinking.\n# Import data d \u0026lt;- read.csv(\u0026#39;post_data/MW_EEG_behavioral.csv\u0026#39;) bads \u0026lt;- c(10, 13, 14) d \u0026lt;- d[!d$subject %in% bads,] ecg \u0026lt;- read.csv(\u0026#39;post_data/MW_ECG_summary.csv\u0026#39;) # Mean imputation on very few missing values d \u0026lt;- d %\u0026gt;% group_by(subject) %\u0026gt;% # Mean imputation mutate(trial = 1:(n()), arou = ifelse(!is.na(arou), arou, mean(arou, na.rm=TRUE))) %\u0026gt;% ungroup() %\u0026gt;% select(subject, trial, arou, fut) # Summarize heart metric heart \u0026lt;- ecg %\u0026gt;% rename(trial = probe) %\u0026gt;% mutate(trial = as.integer(gsub(\u0026#39;Probe\u0026#39;, \u0026#39;\u0026#39;, trial)), timepoint = sample / 250) %\u0026gt;% mutate(hr = 60 / (timepoint - lag(timepoint))) %\u0026gt;% group_by(subject, trial) %\u0026gt;% summarize(hr = mean(hr, na.rm=TRUE)) %\u0026gt;% inner_join(d) ## `summarise()` has grouped output by \u0026#39;subject\u0026#39;. You can override using the ## `.groups` argument. ## Joining with `by = join_by(subject, trial)` # Compute summaries for viz sd \u0026lt;- heart %\u0026gt;% gather(metric, measure, hr, fut) %\u0026gt;% group_by(subject, metric) %\u0026gt;% summarize(r = cor.test(measure, arou)$estimate, ci_l = cor.test(measure, arou)$conf.int[1], ci_h = cor.test(measure, arou)$conf.int[2], p = cor.test(measure, arou)$p.value) %\u0026gt;% mutate(sig = case_when( p \u0026lt; .001 ~ \u0026#39;p \u0026lt; .001\u0026#39;, p \u0026lt; .01 ~ \u0026#39;p \u0026lt; .01\u0026#39;, p \u0026lt; .05 ~ \u0026#39;p \u0026lt; .05\u0026#39;, p \u0026gt;= .05 ~ \u0026#39;N.S.\u0026#39; )) %\u0026gt;% mutate(sig = factor(sig, levels = c(\u0026#39;N.S.\u0026#39;, \u0026#39;p \u0026lt; .05\u0026#39;, \u0026#39;p \u0026lt; .01\u0026#39;, \u0026#39;p \u0026lt; .001\u0026#39;))) ## `summarise()` has grouped output by \u0026#39;subject\u0026#39;. You can override using the ## `.groups` argument. # --- COMPUTE AGGREGATE STATS --- # # Use Fisher Z to transform correlations to unbounded space fisherz \u0026lt;- function(r) { return(0.5 * log((1 + r) / (1 - r))) } sd$z \u0026lt;- fisherz(sd$r) N \u0026lt;- length(unique(sd$subject)) # Aggregate correlation statistics stats \u0026lt;- sd %\u0026gt;% group_by(metric) %\u0026gt;% summarize(m = mean(r), se = sd(r) / sqrt(N), t = t.test(r, mu=0)$statistic, df = t.test(r, mu=0)$parameter, p = t.test(r, mu=0)$p.value, bf = (extractBF(ttestBF(r, mu=0))$bf)) %\u0026gt;% mutate(p_code = ifelse(p \u0026lt; .001, \u0026#39;\u0026lt; .001\u0026#39;, paste0(\u0026#39;= \u0026#39;, round(p, 3))), bf = ifelse(metric==\u0026#39;fut\u0026#39;, bf, 1 / bf)) %\u0026gt;% mutate(r = -1, subject = c(1, 24), label = paste0(\u0026#39;p \u0026#39;, p_code, c(\u0026#39;\\nBF Alt = \u0026#39;, \u0026#39;\\nBF Null = \u0026#39;), round(bf, 2))) metric_order \u0026lt;- c(\u0026#39;hr\u0026#39;, \u0026#39;fut\u0026#39;) stats$metric \u0026lt;- factor(stats$metric, levels = metric_order) stats$metric \u0026lt;- recode(stats$metric, `hr` = \u0026#39;Heart rate\u0026#39;, `fut` = \u0026#39;Future thinking\u0026#39;) colors \u0026lt;- brewer_pal(palette = \u0026#39;RdBu\u0026#39;)(10)[c(1, 10, 8, 6)] # Visualize p3 \u0026lt;- sd %\u0026gt;% mutate(metric = factor(metric, levels = metric_order)) %\u0026gt;% mutate(metric = recode(metric, `hr` = \u0026#39;Heart rate\u0026#39;, `fut` = \u0026#39;Future thinking\u0026#39;)) %\u0026gt;% ggplot(aes(x = reorder_within(subject, r, metric), y = r)) + geom_hline(yintercept = 0, linetype = \u0026#39;dashed\u0026#39;) + geom_errorbar(aes(ymin = ci_l, ymax = ci_h, color = sig), width = .2, linewidth = .5) + geom_point(aes(color = sig), size = .5) + geom_text(data = stats, aes(label=label), size = 3, hjust = 0) + scale_color_manual(values = colors) + labs( x = \u0026#39;Participant\u0026#39;, y = \u0026#39;Within-participant correlation with subjective arousal\u0026#39;, color = \u0026#39;\u0026#39; ) + scale_x_reordered() + scale_y_continuous(breaks = c(-1, 0, 1), labels = c(-1, 0, 1), limits = c(-1, 1)) + coord_flip() + facet_wrap(~metric, scales = \u0026#39;free\u0026#39;, ncol = 3) + theme_bw() + theme(axis.ticks = element_blank(), axis.text.y = element_blank(), strip.background = element_rect(fill = NA), panel.grid = element_blank(), legend.position = c(.39, .3), legend.key.size = unit(.2, \u0026#39;cm\u0026#39;), legend.text = element_text(size = 8), legend.spacing.y = unit(0, \u0026#39;pt\u0026#39;), legend.title = element_blank(), text = element_text(size = 14)) p3 Figure 5: Participant-level correlations between subjective arousal, tendency to think about the future, and heartrate. Bayes factors and p values characterize a one-sample t test against zero.\nWe can see that, for just about all participants, ratings of subjective arousal are largely unrelated to their average heart rate. We can also see that, for many participants, thinking about the future was associated with higher subjective arousal ratings. This makes sense if thinking about the future is energizing or stressful.\n2. Interoceptive sensitivity to cardiac signals is stronger during states of lower subjective arousal To assess whether interoception to cardiac signals changes based on subjective arousal, we examined the results of the cluster-based permutation test, and found one significant cluster. This cluster occurred largely over frontal electrodes about 350 ms post heartbeat.\nlibrary(ggpubr) source(\u0026#39;visualize_clusters.r\u0026#39;) ## ## Attaching package: \u0026#39;arrow\u0026#39; ## The following object is masked from \u0026#39;package:lubridate\u0026#39;: ## ## duration ## The following object is masked from \u0026#39;package:utils\u0026#39;: ## ## timestamp ## ## Attaching package: \u0026#39;eegUtils\u0026#39; ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## filter ## here() starts at /home/dave/Dropbox/post_doc/professional/career/davebraundotnet/blogdown m \u0026lt;- readRDS(\u0026#39;post_data/cluster_result.rds\u0026#39;) cluster_id \u0026lt;- which(m$result$p_values \u0026lt; .05) topo \u0026lt;- plot_topo(m, cluster_id, n_breaks = 1, nrow = 1) ## Joining with `by = join_by(channel)` ## Warning in geom_topo(chan_markers = \u0026#34;text\u0026#34;, aes(fill = t_statistic, label = ## electrode)): Ignoring unknown aesthetics: label ## Warning in geom_topo(chan_markers = \u0026#34;text\u0026#34;, aes(fill = t_statistic, label = electrode)): Ignoring unknown aesthetics: fill and label ## Ignoring unknown aesthetics: fill and label ## Ignoring unknown aesthetics: fill and label ## Warning in geom_topo(chan_markers = \u0026#34;text\u0026#34;, aes(fill = t_statistic, label = ## electrode)): Ignoring unknown aesthetics: fill ## Warning in geom_topo(chan_markers = \u0026#34;text\u0026#34;, aes(fill = t_statistic, label = ## electrode)): Ignoring unknown aesthetics: fill and label channels \u0026lt;- m$result$channels[m$result$clusters[[2]][[2]] + 1] times \u0026lt;- m$result$times[m$result$clusters[[2]][[1]] + 1] d \u0026lt;- m$eeg N \u0026lt;- length(unique(d$subject)) pd \u0026lt;- d %\u0026gt;% gather(channel, voltage, Fp1:POz) %\u0026gt;% filter(channel %in% channels) %\u0026gt;% group_by(subject, condition, time) %\u0026gt;% summarize(voltage_ = mean(voltage)) %\u0026gt;% group_by(condition, time) %\u0026gt;% summarize(voltage = mean(voltage_), se = sd(voltage_) / sqrt(N)) ## `summarise()` has grouped output by \u0026#39;subject\u0026#39;, \u0026#39;condition\u0026#39;. You can override ## using the `.groups` argument. ## `summarise()` has grouped output by \u0026#39;condition\u0026#39;. You can override using the ## `.groups` argument. rib \u0026lt;- pd %\u0026gt;% filter(time %in% times) %\u0026gt;% group_by(time) %\u0026gt;% summarize(ymin = min(voltage, na.rm = TRUE), ymax = max(voltage, na.rm = TRUE)) low \u0026lt;- \u0026#39;#FC8D59\u0026#39; high \u0026lt;- \u0026#39;#91BFDB\u0026#39; ts \u0026lt;- pd %\u0026gt;% ggplot(aes(x = time, y = voltage)) + geom_rect(aes(xmin = .25, xmax = .650, ymin = min(voltage), ymax = max(voltage)), color = \u0026#39;steelblue\u0026#39;, fill = NA, linetype = \u0026#39;dashed\u0026#39;) + geom_ribbon(data = rib, aes(ymin = ymin, ymax = ymax, xmin = times[1], xmax = times[length(times)], y=1), alpha = .6, fill = \u0026#39;green\u0026#39;) + geom_ribbon(aes(ymin = voltage - se, ymax = voltage + se, fill = condition), alpha = .3) + geom_line(aes(color = condition)) + labs( title = \u0026#39;Heartbeat evoked potential (HEP)\u0026#39;, x = \u0026#39;Time post heartbeat (s)\u0026#39;, y = latex2exp::TeX(\u0026#39;$EEG Voltage~ (\\\\mu ~V)$\u0026#39;), color = \u0026#39;Subjective arousal\u0026#39;, fill = \u0026#39;Subjective arousal\u0026#39; ) + scale_color_manual(values = c(Deactivated = low, Activated = high)) + scale_fill_manual(values = c(Deactivated = low, Activated = high)) + theme_bw() + theme(axis.ticks = element_blank(), panel.grid = element_blank(), legend.position = \u0026#39;bottom\u0026#39;) g \u0026lt;- ggarrange(ts, topo, labels = c(\u0026#39;A.\u0026#39;, \u0026#39;B.\u0026#39;), nrow=2) print(g) Figure 6: Main result of cluster-based permutation analysis. (A) The grand averaged timeseries of EEG voltage timelocked to the heartbeat (ie, the HEP). Blue dashed rectangle highlights the time points that were analyzed in the cluster test. Green shaded area highlights the significant cluster. (B) Topographical heat map of t values during the significant time window (at time point 0.328 s post heartbeat). The significant cluster centers on frontal electrodes.\nFigure 6 highlights the significant cluster from the cluster-based permutation analysis. We can see that, on average, EEG voltage was higher (ie, stronger interoception) prior to lower ratings of subjective arousal. This suggests that participants were more sensitive to cardiac signals during periods of lower subjective arousal.\n3. The link between interoceptive sensitivity and subjective arousal was amplified for those high in state anxiety Given the strong links between anxiety and interoception1, we next wanted to look at how this HEP effect we found might be different for those with low vs. high anxiety. At the beginning of the experiment, we collected measures of both trait anxiety (ie, how much anxiety one experiences generally) and state anxiety (ie, how much anxiety one is experience while completing the survey).\nIf I were to plot that significant green window in Figure 6 for each participant, we would see that each participant is different with respect to how strong their link is between interoception and subjective arousal (ie, their HEP effect). We wanted to know whether these differences could be explained by differences in state or trait anxiety.\nSo, for each participant, I calculated their average HEP effect and correlated it with their anxiety measures:\n# --- TOP PANEL --- # # Individual-level correlations between HEP effect size and survey response # Select correct data channels_s \u0026lt;- m$result$channels[m$result$clusters[[2]][[2]] + 1] times_s \u0026lt;- m$result$times[m$result$clusters[[2]][[1]] + 1] cluster \u0026lt;- data.frame(time = times_s, channel = channels_s) cluster$string \u0026lt;- paste(cluster$time, cluster$channel, sep=\u0026#39;_\u0026#39;) survey \u0026lt;- read.csv(\u0026#39;post_data/MW_EEG_survey.csv\u0026#39;) survey \u0026lt;- survey[, c(\u0026#39;subj_id\u0026#39;, \u0026#39;GAD7_Score\u0026#39;, \u0026#39;STAI_Score\u0026#39;)] colnames(survey) \u0026lt;- c(\u0026#39;subject\u0026#39;, \u0026#39;gad\u0026#39;, \u0026#39;stai\u0026#39;) d \u0026lt;- m$eeg # Select appropriate (significant) cluster hep_s \u0026lt;- d %\u0026gt;% gather(channel, voltage, Fp1:POz) %\u0026gt;% mutate(string = paste(time, channel, sep=\u0026#39;_\u0026#39;)) %\u0026gt;% filter(string %in% cluster$string, channel %in% channels_s) %\u0026gt;% group_by(subject, condition) %\u0026gt;% summarize(voltage = mean(voltage)) %\u0026gt;% mutate(condition = ifelse(condition == \u0026#39;Activated\u0026#39;, \u0026#39;High Arousal\u0026#39;, \u0026#39;Low Arousal\u0026#39;)) %\u0026gt;% inner_join(survey) ## `summarise()` has grouped output by \u0026#39;subject\u0026#39;. You can override using the ## `.groups` argument. ## Joining with `by = join_by(subject)` # Make general summarized data t \u0026lt;- hep_s %\u0026gt;% spread(condition, voltage) %\u0026gt;% mutate(hep = `Low Arousal` - `High Arousal`) %\u0026gt;% select(-`High Arousal`, -`Low Arousal`) %\u0026gt;% gather(survey, response, gad,stai) %\u0026gt;% mutate(survey = recode(survey, `gad` = \u0026#39;Trait anxiety\u0026#39;, `stai` = \u0026#39;State anxiety\u0026#39;)) # Compute group-level correlations cr \u0026lt;- t %\u0026gt;% group_by(survey) %\u0026gt;% summarize(r = cor.test(hep, response)$estimate, df = cor.test(hep, response)$parameter, p = cor.test(hep, response)$p.value, ci_h = cor.test(hep, response)$conf.int[1], ci_l = cor.test(hep, response)$conf.int[2]) %\u0026gt;% mutate(label = paste0(\u0026#39;r(\u0026#39;, df, \u0026#39;) = \u0026#39;, round(r, 3), \u0026#39;\\n95% CI = [\u0026#39;, round(ci_l, 3), \u0026#39;, \u0026#39;, round(ci_h, 3), \u0026#39;]\\np = \u0026#39;, round(p, 3)), hep = c(-.5, -.5), response = c(39, 10), survey = c(\u0026#39;State anxiety\u0026#39;, \u0026#39;Trait anxiety\u0026#39;)) # Visualize correlations p1 \u0026lt;- t %\u0026gt;% filter(!is.na(response)) %\u0026gt;% ggplot(aes(x = response, hep)) + geom_point() + geom_smooth(method = \u0026#39;lm\u0026#39;) + facet_wrap(~survey, scales = \u0026#39;free_x\u0026#39;) + geom_text(data = cr, aes(label = label), hjust = 0, size = 3) + labs( x = \u0026#39;Survey response\u0026#39;, y = \u0026#39;Participant-specific\\nHEP effect size\u0026#39; ) + ylim(-1, 1) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = NA, color = \u0026#39;black\u0026#39;), axis.ticks = element_blank()) # --- PANEL 2 --- # # Visualize median split on EEG timeseries # Keep relevant columns channels_s \u0026lt;- unique(channels_s) d \u0026lt;- m$eeg mask1 \u0026lt;- colnames(d) %in% c(\u0026#39;subject\u0026#39;, \u0026#39;condition\u0026#39;, \u0026#39;time\u0026#39;) mask2 \u0026lt;- colnames(d) %in% channels_s d \u0026lt;- d[, (mask1 | mask2)] # Plot p2 \u0026lt;- d %\u0026gt;% inner_join(survey[,c(\u0026#39;subject\u0026#39;, \u0026#39;stai\u0026#39;)]) %\u0026gt;% filter(!is.na(stai), time %in% times) %\u0026gt;% mutate(stai_d = ifelse(stai \u0026gt; median(stai), \u0026#39;High state anxiety\u0026#39;, \u0026#39;Low state anxiety\u0026#39;)) %\u0026gt;% gather(channel, voltage, Fp1:FC5) %\u0026gt;% group_by(subject, time, condition, stai_d) %\u0026gt;% summarize(voltage_ = mean(voltage)) %\u0026gt;% group_by(time, condition, stai_d) %\u0026gt;% summarize(voltage = mean(voltage_), se = sd(voltage_) / sqrt(n())) %\u0026gt;% mutate(stai_d = factor(stai_d, levels = c(\u0026#39;Low state anxiety\u0026#39;, \u0026#39;High state anxiety\u0026#39;))) %\u0026gt;% ggplot(aes(x = time, y = voltage)) + geom_ribbon(aes(ymin = voltage - se, ymax = voltage + se, fill = condition), alpha = .3) + geom_line(aes(color = condition)) + facet_wrap(~stai_d) + labs( x = \u0026#39;Time post heartbeat (s)\u0026#39;, y = latex2exp::TeX(\u0026#39;$EEG~voltage~(\\\\mu ~V)$\u0026#39;), fill = \u0026#39;Subjective arousal\u0026#39;, color = \u0026#39;Subjective arousal\u0026#39; ) + scale_color_manual(values = c(`Activated` = high, `Deactivated` = low)) + scale_fill_manual(values = c(`Activated` = high, `Deactivated` = low)) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = NA, color = \u0026#39;black\u0026#39;), axis.ticks = element_blank(), legend.position = \u0026#39;bottom\u0026#39;) ## Joining with `by = join_by(subject)` ## `summarise()` has grouped output by \u0026#39;subject\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;condition\u0026#39;. You can ## override using the `.groups` argument. ## `summarise()` has grouped output by \u0026#39;time\u0026#39;, \u0026#39;condition\u0026#39;. You can override using ## the `.groups` argument. g \u0026lt;- ggarrange(p1, p2, labels = c(\u0026#39;A.\u0026#39;, \u0026#39;B.\u0026#39;), nrow = 2) ## `geom_smooth()` using formula = \u0026#39;y ~ x\u0026#39; ## Warning: Removed 2 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 2 rows containing missing values (`geom_point()`). print(g) Figure 7: Participant-level associations between HEP effect size and self-reported anxiety ratings. (A) Correlations with HEP effect size for both state (left) and trait (right) anxiety. (B) EEG grand averaged timeseries broken by a median split on state anxiety.\nWe can see that for state (but not trait) anxiety, there is a significant, positive correlation with HEP effect size (Figure 7). This suggests that, as people are feeling more anxious in the moment, their sensitivity to cardiac signals is more coupled with their momentary changes in subjective arousal. What it means As your subjective arousal increases, your brain becomes more sensitive to your heart signals\u0026mdash;but if you\u0026rsquo;re higher in anxiety, that boost in sensitivity is even stronger We found that the brain\u0026rsquo;s response to heartbeats changes depending on how energized you feelâand that this effect is different for people with higher anxiety. When people were in a more amped-up state (what weâre calling high subjective arousal), their brains showed a stronger reaction to heartbeats. And for folks with high anxiety, this pattern was amplified.\nSo what does that mean, practically?\nIt suggests that when your energy level shiftsâeven just quietly, while youâre restingâyour brain is tuning in to your body in a different way. And if youâre someone who struggles with anxiety, your brain might be tuning in differently altogether.\nThis matters because a lot of anxiety happens at rest, when weâre left alone with our thoughts. The brain networks that light up during rest are also the ones tied to rumination, worry, and âwhat ifâ spirals. Our results suggest that these moments of rest may not be as neutral as they seem: your brain is actively tracking how your body feels, and if you\u0026rsquo;re anxious, it might be tracking differently.\nIn the future, this kind of research could inform better mental health tools. Imagine a wearable that doesnât just track your heart rate, but notices when your brainâs response to your body changes in a way that predicts anxiety. Or an app that could nudge you toward a calming activity right when your internal signals suggest you\u0026rsquo;re about to start spiraling.\nMore broadly, this work shows that our internal worldâhow we feel insideâhas a real, measurable footprint in the brain. It reinforces the idea that emotions arenât just things we talk about in therapyâtheyâre deeply embedded in how our body and brain talk to each other. And that connection might be key to understanding not just anxiety, but how we can better support mental health in everyday life.\nBack to homepage.\nPang, J., Tang, X., Li, H., Hu, Q., Cui, H., Zhang, L., Li, W., Zhu, Z., Wang, J., \u0026amp; Li, C. (2019). Altered Interoceptive Processing in Generalized Anxiety DisorderâA Heartbeat-Evoked Potential Research. Frontiers in Psychiatry, 10, 616. https://doi.org/10.3389/fpsyt.2019.00616\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nColl, M.-P., Hobson, H., Bird, G., \u0026amp; Murphy, J. (2021). Systematic review and meta-analysis of the relationship between the heartbeat-evoked potential and interoception. Neuroscience \u0026amp; Biobehavioral Reviews, 122, 190â200. https://doi.org/10.1016/j.neubiorev.2020.12.012\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFourcade, A., Klotzsche, F., Hofmann, S. M., Mariola, A., Nikulin, V. V., Villringer, A., \u0026amp; Gaebler, M. (2024). Linking brainâheart interactions to emotional arousal in immersive virtual reality. Psychophysiology, e14696. https://doi.org/10.1111/psyp.14696\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKucyi, A., Kam, J. W. Y., Andrews-Hanna, J. R., Christoff, K., \u0026amp; Whitfield-Gabrieli, S. (2023). Recent advances in the neuroscience of spontaneous and off-task thought: Implications for mental health. Nature Mental Health, 1(11), 827â840. https://doi.org/10.1038/s44220-023-00133-w\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPark, H.-D., \u0026amp; Blanke, O. (2019). Heartbeat-evoked cortical responses: Underlying mechanisms, functional roles, and methodological considerations. NeuroImage, 197, 502â511. https://doi.org/10.1016/j.neuroimage.2019.04.081\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n", 
    "url": "\/\/localhost:4321\/2025\/04\/24\/does-the-brain-cope-with-anxiety-by-numbing-out\/"
        },
    
    
    "\/\/localhost:4321\/post\/": {
        
        "title": "Posts",
            "tags": [],
    "content": "", 
    "url": "\/\/localhost:4321\/post\/"
        },
    
    
    "\/\/localhost:4321\/2022\/03\/20\/forecasting-covid-with-a-little-help-from-your-neighbors.\/": {
        
        "title": "Forecasting COVID with a little help from your neighbors.",
            "tags": [],
    "content": " ð Read the Paper\nð¼ View the Poster\nð¾ Download the data\nTL;DR I built models that accurately predicted COVID-19 case numbers up to three weeks aheadâusing not just case data, but also how well people thought their communities were following safety guidelines like social distancing. This approach shows how public perception can be used to improve real-time forecasting tools for organizations like the CDC.\nKey Skills ð Time-Series Forecasting â Applied epidemiological models to predict COVID-19 cases using behavioral and clinical data\nð§  Bayesian Modeling â Leveraged probabilistic forecasts to support real-time public health decision-making\nð Multimodal Data Integration â Combined self-reported survey data with case count time series to enhance model accuracy\nð Communicating Uncertainty â Created clear, decision-relevant visualizations for probabilistic outcomes and behavioral drivers\nWhat I Learned How to turn crowdsourced human behavior data into probabilistic forecasts using Bayesian modelingâproviding community leaders and health systems with more informative, uncertainty-aware predictions of epidemic spread.\nWhat we were interested in What if we could improve pandemic forecasting by asking people how well their neighbors are following the rules? Modeling the path of a fast-moving disease like COVID-19 is a popular and important area of research1,2. Infectious disease models have become essential tools for public healthâhelping experts make sense of whatâs happening now and prepare for what might happen next. 3,4 Most of these models are purely computationalâthey crunch numbers from online data sources like case counts, hospitalizations, or mobility trends to make their best guess at the future. 5,6\nBut human judgment can really boost predictive power. Whether itâs gut-level forecasts or indirect clues like social media posts, human behavior has a proven track record of boosting infectious disease predictions. 7,8,9,10 So we wondered: What if we tapped into something even simplerâpeopleâs sense of whether their neighbors were actually following CDC guidelines? Could that kind of judgment help our models do a better job predicting where COVID-19 was headed?\nCan peopleâs perceptions of public health behavior help us figure out which COVID-19 interventions actually workedâand make our forecasts better in the process?\nDuring the COVID-19 pandemic, the CDC issued many non-pharmaceutical interventions (NPIs), or behavior-based ways of mitigating the spread of the disease (eg, social distancing). During the pandemic, Iâm sure we all had some intuitions around whether people were adhering to these NPIs, how that adherence changed throughout the course of the pandemic, and how effective that NPI might have been for reducing the spread of the disease. In this study, we wanted to know whether the degree to which people were adhering to NPIs could improve predictions of infectious disease spread, and see which NPIs improved predictions the most.\nFigure 1: Can we better predict the spread of disease if we can measure how many people are wearing masks or social distancing?\nHow we did it We asked a crowd 21 questions about their communityâs compliance with NPIs over 35 weeks and tested whether their responses improved COVID-19 case forecasts During the COVID-19 pandemicâfrom August 2020 through April 2021âwe sent surveys to people across the US asking them 21 core questions about how well their communities were complying with the CDCâs NPI regulations (see Figure 2).\nFigure 2: The 21 survey questions posed to the crowd. Responses were on a Likert scale from 0% to 100% in increments of 20. Participants also had the option of selecting âDonât knowâ.\nWe collected a total of 10,852 survey responses across three different survey platforms. These responses were evenly distributed over time and roughly geographically representative of the US population (see Figures 3 and 4).\nYou can interact with these plots!\ndates \u0026lt;- unique(d$year_month_day) dates \u0026lt;- dates[order(unique(d$mw))] dates \u0026lt;- ifelse((seq_along(dates)+3) %% 4 == 0, dates, \u0026#34;\u0026#34;) p1 \u0026lt;- d %\u0026gt;% group_by(mw, survey_platform) %\u0026gt;% summarize(year_month_day = unique(year_month_day), count = length(unique(id))) %\u0026gt;% mutate(survey_platform = recode(survey_platform, `sm_volunteer` = \u0026#39;Platform A\u0026#39;, `sm_paid` = \u0026#39;Platform B\u0026#39;, `pollfish` = \u0026#39;Platform C\u0026#39;)) %\u0026gt;% ggplot(aes(x = reorder(year_month_day, mw), y = count, color=survey_platform, group=survey_platform, text=paste0(\u0026#39;Date: \u0026#39;, year_month_day, \u0026#39;\u0026lt;br\u0026gt;Number of responses: \u0026#39;, count, \u0026#39;\u0026lt;br\u0026gt;Survey platform: \u0026#39;, survey_platform))) + geom_line() + geom_point() + labs( x = \u0026#39;Date\u0026#39;, y = \u0026#39;Number of responses\u0026#39;, color = \u0026#39;Survey platform\u0026#39; ) + ylim(0, 450) + scale_x_discrete(labels = dates, breaks = dates) + scale_color_manual(values = qual[c(1, 4, 5)]) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \u0026#39;bottom\u0026#39;, text = element_text(size = 16)) ## `summarise()` has grouped output by \u0026#39;mw\u0026#39;. You can override using the `.groups` ## argument. pp1 \u0026lt;- ggplotly(p1, tooltip=\u0026#39;text\u0026#39;) pp1 %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, # horizontal legend x = 0.5, # centered horizontally xanchor = \u0026#34;center\u0026#34;, y = -0.5, # move below the plot area yanchor = \u0026#34;top\u0026#34; ) ) Figure 3: Data collected across three survey platforms over time.\np2 \u0026lt;- census %\u0026gt;% mutate(state_title = str_to_title(state), expected = pop_prop * N) %\u0026gt;% relocate(expected, .after = observed) %\u0026gt;% mutate(diff = abs(observed - expected)) %\u0026gt;% mutate(outlier = ifelse(diff \u0026gt; quantile(diff, probs = .9), \u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;)) %\u0026gt;% mutate(outlier_label = ifelse(outlier == \u0026#39;yes\u0026#39;, state_title, NA)) %\u0026gt;% ggplot(aes(x = expected, y = observed)) + geom_abline(slope = 1, intercept = 0, linetype = \u0026#39;dashed\u0026#39;) + geom_point(aes(color = outlier, text = paste0(\u0026#39;Expected: \u0026#39;, round(expected), \u0026#39;\u0026lt;br\u0026gt;Observed: \u0026#39;, observed, \u0026#39;\u0026lt;br\u0026gt;State: \u0026#39;, state_title))) + geom_text(aes(label = outlier_label, text = NULL), nudge_y = 40) + ylim(0, 1200) + xlim(0, 1200) + labs( x = \u0026#39;Expected responses\u0026#39;, y = \u0026#39;Observed responses\u0026#39; ) + scale_color_manual(values = c(`no` = qual[1], `yes` = qual[3])) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks = element_blank(), legend.position = \u0026#39;none\u0026#39;, text = element_text(size = 16)) ## Warning in geom_point(aes(color = outlier, text = paste0(\u0026#34;Expected: \u0026#34;, ## round(expected), : Ignoring unknown aesthetics: text pp2 \u0026lt;- ggplotly(p2, tooltip=\u0026#39;text\u0026#39;) pp2 Figure 4: Actual vs.Â expected number of responses by state.\nWe used perceptions of community behavior to predict COVID-19 cases So, in total, we have 10,852 survey responses, each containing answers to 21 questions about community adherence to NPIs over 35 weeks during the COVID-19 pandemic. We took two approaches to making better COVID-19 predictions based on these responses.\nCorrelations\nWe first looked at correlations between each question and incident (ie, new) COVID-19 from 0 - 4 weeks ahead. These correlations gave us a basic sense of which community behavior might be most important in predicting future casesâa great starting point.\nProbabilistic forecasting\nCorrelations are great, but they donât give us a predicted number of cases on a future date, which is the type of information that decision makers like CDC would need to support situational awareness during a pandemic. Enter probabilistic forecasting.\nProbabilistic forecasts can take in everything we know about previous cases and peopleâs sense of whatâs happening in their community, and they can spit out a specific number of COVID-19 cases to expect on a future date. Even better than that, they can estimate the uncertainty around the prediction, which is really important! Imagine our model tells us to expect 1,000 new COVID-cases next week. Itâs a huge difference if the uncertainty around that perception says 1,000 new cases give or take 10 cases (very precise!), or 1,000 new cases give or take 1,000 (pretty much useless!).\nWe tried three different approaches to probabilistic forecasting:\nBaseline model: This is what it sounds like. This model includes no information from our surveys about community NPI compliance; it only relies on previous case counts to predict future cases. This is often used as a âfirst guessâ in many modeling applications, and so it serves as our baseline or control approach here. If including community compliance data canât beat this model, this data may not be useful after all.\nSingle question model: To predict future COVID-19 cases, this model takes into account all previous case counts and responses to one of the community compliance survey questions. We technically ran one of these models for each question, and comparing how well each of these models does can tell us how important each type of compliance behavior is for predicting future cases.\nFull question model: This one pulls out all the stopsâit uses past case counts and every single community compliance survey question. Itâs the âmac daddyâ model. The upside? It captures the full picture of how people say theyâre behaving. The downside? If too many of those questions donât actually relate to case trends, the model might end up learning noise instead of signalâbasically, getting really good at predicting nonsense.\nWhat we found Two questions reliably correlated with cases several weeks ahead Figure 5 shows questions that are positively and negatively related to future COVID-19 cases:\npd1 \u0026lt;- jhu %\u0026gt;% mutate(inccases_lead1 = lead(inccases), inccases_lead2 = lead(inccases, n = 2), inccases_lead3 = lead(inccases, n = 3), inccases_lead4 = lead(inccases, n = 4)) %\u0026gt;% select(EW, contains(\u0026#39;lead\u0026#39;)) %\u0026gt;% rename(ew = EW) %\u0026gt;% inner_join(d) ## Joining with `by = join_by(ew)` coding \u0026lt;- coding %\u0026gt;% mutate(question = as.integer(str_replace(question, \u0026#39;q\u0026#39;, \u0026#39;\u0026#39;))) %\u0026gt;% select(question, question_content) pd2 \u0026lt;- pd1 %\u0026gt;% gather(lead, cases, inccases, contains(\u0026#39;lead\u0026#39;)) %\u0026gt;% mutate(lead = recode(lead, `inccases` = 0, `inccases_lead1` = 1, `inccases_lead2` = 2, `inccases_lead3` = 3, `inccases_lead4` = 4)) %\u0026gt;% inner_join(coding) %\u0026gt;% group_by(ew, question, lead) %\u0026gt;% summarize(response = mean(response, na.rm = TRUE), question_content=unique(question_content), cases = unique(cases)) %\u0026gt;% group_by(question_content, lead) %\u0026gt;% summarize(r = cor.test(response, cases)$estimate, ci_l = cor.test(response, cases)$conf.int[1], ci_h = cor.test(response, cases)$conf.int[2], p = cor.test(response, cases)$p.value) %\u0026gt;% mutate(p = ifelse(p \u0026lt; .001, \u0026#39;p \u0026lt; .001\u0026#39;, paste0(\u0026#39;p = \u0026#39;, round(p, 3)))) %\u0026gt;% mutate(label = paste0(\u0026#39;r = \u0026#39;, round(r, 3), \u0026#39;\\n95CI = [\u0026#39;, round(ci_l, 3), \u0026#39;, \u0026#39;, round(ci_h, 3), \u0026#39;]\\n\u0026#39;, p)) ## Joining with `by = join_by(question)` ## `summarise()` has grouped output by \u0026#39;ew\u0026#39;, \u0026#39;question\u0026#39;. You can override using ## the `.groups` argument. ## `summarise()` has grouped output by \u0026#39;question_content\u0026#39;. You can override using ## the `.groups` argument. pd3 \u0026lt;- pd2 %\u0026gt;% group_by(question_content) %\u0026gt;% summarize(avg_r = mean(r)) %\u0026gt;% inner_join(pd2) ## Joining with `by = join_by(question_content)` p1 \u0026lt;- pd3 %\u0026gt;% ggplot(aes(x = lead, y = reorder(question_content, avg_r))) + geom_tile(aes(fill = r, text = label)) + scale_fill_gradientn(colors = sequ) + labs( x = \u0026#39;Weeks ahead\u0026#39;, y = \u0026#39;Survey question\u0026#39;, fill = \u0026#39;Correlation\\ncoefficient\u0026#39; ) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks = element_blank()) ## Warning in geom_tile(aes(fill = r, text = label)): Ignoring unknown aesthetics: ## text pp3 \u0026lt;- ggplotly(p1, tooltip=\u0026#39;text\u0026#39;) pp3 Figure 5: Correlations between responses to each survey question and incident COVID-19 cases from 0 - 4 weeks ahead.\nColleges holding remote classes was associated with higher present and future COVID-19 cases (which was âsignificantâ up to 2 weeks ahead). Conversely, people practing more social distancing was associated with fewer present and future COVID-19 cases. Interestingly, some behaviors, such as mask wearing, werenât associated with cases at all.\nModel including all questions performed the best Below, in Figure 6, you can see how the different models we tested stack up against each other.\ncases \u0026lt;- jhu[jhu$EW %in% unique(d$ew), c(\u0026#39;EW\u0026#39;, \u0026#39;inccases\u0026#39;)] colnames(cases) \u0026lt;- tolower(colnames(cases)) cases$forecast_week \u0026lt;- 1:nrow(cases) mw \u0026lt;- 1:(length(unique(cases$ew))) mw_ew \u0026lt;- data.frame(mw = mw, ew = unique(cases$ew)) mw_ew \u0026lt;- inner_join(mw_ew, unique(d[, c(\u0026#39;ew\u0026#39;, \u0026#39;year_month_day\u0026#39;)])) ## Joining with `by = join_by(ew)` preds_ \u0026lt;- preds %\u0026gt;% inner_join(cases[, c(\u0026#39;forecast_week\u0026#39;, \u0026#39;ew\u0026#39;)]) %\u0026gt;% inner_join(mw_ew) %\u0026gt;% mutate(mw = mw + step) ## Joining with `by = join_by(forecast_week)` ## Joining with `by = join_by(ew)` cases \u0026lt;- inner_join(cases, mw_ew) ## Joining with `by = join_by(ew)` n_x \u0026lt;- 5 n_y \u0026lt;- 3 mw \u0026lt;- unique(mw_ew$mw) x_labs \u0026lt;- mw_ew$year_month_day[seq(1, nrow(mw_ew), by = n_x)] x_breaks \u0026lt;- mw[seq(1, length(mw), by = n_x)] y_breaks \u0026lt;- seq(0, round(max(cases$inccases)) + 1000000, length.out = 5) keep_questions \u0026lt;- c(23, 2, 10, 19, 20, 99) levels \u0026lt;- coding[coding$question %in% keep_questions,]$question_content levels \u0026lt;- c(\u0026#39;Baseline model\u0026#39;, levels, \u0026#39;Full model\u0026#39;) p \u0026lt;- preds_ %\u0026gt;% rename(question = QS) %\u0026gt;% filter(quantile %in% c(0.025, .5, .975), question %in% keep_questions) %\u0026gt;% left_join(coding) %\u0026gt;% mutate(question_content = case_when( question == 99 ~ \u0026#39;Full model\u0026#39;, question == 23 ~ \u0026#39;Baseline model\u0026#39;, TRUE ~ question_content )) %\u0026gt;% mutate(quantile = recode(quantile, `0.025` = \u0026#39;ci_l\u0026#39;, `0.5` = \u0026#39;pred\u0026#39;, `0.975` = \u0026#39;ci_h\u0026#39;), question_content = factor(question_content, levels=levels)) %\u0026gt;% spread(quantile, value) %\u0026gt;% ggplot(aes(x = mw, y = pred)) + geom_line(data = cases, aes(x = mw, y = inccases)) + geom_line(aes(group = forecast_week), color = qual[4]) + geom_point(aes(group = forecast_week, text = paste0(\u0026#39;Week: \u0026#39;, year_month_day, \u0026#39;\\nCases: \u0026#39;, round(pred), \u0026#39;\\n95% CI Lower: \u0026#39;, round(ci_l), \u0026#39;\\n95% CI Upper: \u0026#39;, round(ci_h))), color = qual[4]) + geom_ribbon(aes(ymin = ci_l, ymax = ci_h, group = forecast_week), alpha = .4, fill = qual[4]) + facet_wrap(~question_content) + scale_x_continuous(labels = x_labs, breaks = x_breaks) + scale_y_continuous(breaks=y_breaks, labels = scientific) + labs( x = \u0026#39;\u0026#39;, y = \u0026#39;Incident COVID-19 cases\u0026#39; ) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.minor = element_blank(), strip.background = element_rect(fill = NA), panel.spacing.y = unit(1, \u0026#39;line\u0026#39;)) ## Joining with `by = join_by(question)` ## Warning in geom_point(aes(group = forecast_week, text = paste0(\u0026#34;Week: \u0026#34;, : ## Ignoring unknown aesthetics: text ggplotly(p, tooltip=\u0026#39;text\u0026#39;) %\u0026gt;% layout(margin = list(b = 50), xaxis = list(title = list(text = \u0026#39;Epidemic week\u0026#39;, standoff = 10))) Figure 6: Model forecasts across epidemic weeks and model types. Black line represents observed cases. Green lines and bands represent model predictions and uncertainty, repsectively.\nEach facet shows a different model we tested. The first and last models are like bookends: the baseline model ignores the survey entirely, while the full model uses every survey question (only the best performing questions are included here).\nThe black line shows the actual number of new COVID cases over time. The green line is what the model predicted, and the shaded band around it is the modelâs uncertaintyâits way of saying, âgive or take a bit.â\nIf the green line closely hugs the black one, the model nailed it. If the black line stays inside the green band, that means the model gave a solid estimate with a realistic sense of uncertainty. Thatâs a win in forecasting land.\nYou can see in Figure 6 that the period where new cases start to decline after the peak (~ 2021-01-23) is the hardest part to predict. Only the model including the social distancing question and the full model seemed to get close during this period. When we did a full quantitative comparison, the full model actually performed the best out of all of them.\nWe can also quantitatively compare the different models including only one question to get a sense of which question improved forecasting the most (Figure 7).\np \u0026lt;- wis %\u0026gt;% mutate(question = questionset_x + 1) %\u0026gt;% inner_join(coding) %\u0026gt;% group_by(question_content) %\u0026gt;% mutate(prop_m = mean(prop), week_ahead = paste0(week_ahead, \u0026#39; week(s) ahead\u0026#39;)) %\u0026gt;% ungroup() %\u0026gt;% filter(prop_m \u0026gt;= quantile(prop_m, probs = .75)) %\u0026gt;% ggplot(aes(x = reorder(question_content, prop), y = prop)) + geom_hline(yintercept = .5, linetype = \u0026#39;dashed\u0026#39;) + geom_point(color = qual[4], aes(text = paste0(\u0026#39;Proportion improved: \u0026#39;, round(prop, 3)))) + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0, color = qual[4]) + facet_wrap(~week_ahead) + coord_flip() + ylim(0, 1) + labs( x = \u0026#39;Survey question\u0026#39;, y = \u0026#39;Proportion WIS score improved\u0026#39;, caption = \u0026#34;Improvement is relative to baseline model.\\n0.5 indicates the one model wasn\u0026#39;t better than the other.\u0026#34; ) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks = element_blank(), strip.background = element_rect(fill = NA), panel.spacing.y = unit(1, \u0026#39;line\u0026#39;)) ## Joining with `by = join_by(question)` ## Warning in geom_point(color = qual[4], aes(text = paste0(\u0026#34;Proportion improved: ## \u0026#34;, : Ignoring unknown aesthetics: text ggplotly(p, tooltip=\u0026#39;text\u0026#39;) Figure 7: Model fit (WIS) scores across models including one question at different forecast ranges.\nâImprovementâ in Figure 7 means improvement relative to the baseline model (not including any questions). An improvement of 0.5 means no model had an advantage. If the band around the dot doesnât include 0.5, that means the model including the question did a better job forecasting than the baseline model.\nHere we can see that a handful of questions were quite effective for predicting 1 to 2 weeks ahead, fewer questions were able to predict 3 weeks ahead (eg, staying home), and no question was great for predicting 4 weeks ahead.\nWhat it means So, whatâs the big takeaway from all this?\nTurns out, if you want to make better predictions during a pandemic, it might actually help to ask regular people a simple question: âAre folks around you following the rules?â\nWe saw from our correlations and probabilistic forecasts that asking people whether their community is practicing social distancing or whether colleges are holding remote classes are more important predictors of future cases than something like mask wearing.\nEven though these kinds of perceptions arenât perfectâeveryone sees the world a little differentlyâthey still captured real, useful signals about how the virus was spreading. And when we threw those signals into a forecasting model, the model got better. Not just a little better in some fluke-y way, but consistently better, especially when we focused on behaviors that really seemed to matterâlike social distancing.\nWhatâs even cooler is that these were crowdsourced perceptions. Not some official metric, not a perfectly measured compliance rate. Just people giving their take on what was happening around them.\nOne interesting wrinkle here is what it really means when a survey question helps the model make better predictions. Does that mean the behavior in questionâlike social distancingâis actually more effective at slowing the spread of disease than something like getting tested? Not necessarily. It could just be that some behaviors, like social distancing, are easier for people to notice and judge in their communities. In contrast, things like testing rates might be harder to observe, even if theyâre equally or more important. This highlights a key feature of using crowdsourced perceptions for forecasting: the best predictors are likely to be behaviors that are both visible to the average person and meaningful for transmission. That sweet spot is where human judgment can really shine.\nIn a world where data gaps are inevitableâespecially early in a pandemicâthis kind of âhuman sensor networkâ could be a surprisingly powerful tool. It wonât replace traditional data sources, but it can fill in the cracks, add nuance, and maybe even give forecasters a bit of an edge when it really counts.\nThe full model wasnât perfect (no model ever is), but it showed that collective human judgment has a place in high-stakes public health forecasting. It showed that maybe your take on whatâs happening around you is more valuable than you think.\nChelsea S Lutz, Mimi P Huynh, Monica Schroeder, Sophia Anyatonwu, F Scott Dahlgren, Gregory Danyluk, Danielle Fernandez, Sharon K Greene, Nodar Kipshidze, Leann Liu, et al.Â Applying infectious disease forecasting to public health: a path forward using influenza forecasting examples. BMC Public Health, 19(1):1â12, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSimon Pollett, Michael A Johansson, Nicholas G Reich, David Brett-Major, Sara Y Del Valle, Srinivasan Venkatramanan, Rachel Lowe, Travis Porco, Irina Maljkovic Berry, Alina Deshpande, et al.Â Recommended reporting items for epidemic forecasting and prediction research: The epiforge 2020 guidelines. PLoS medicine, 18(10):e1003793, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMatthew Biggerstaff, Rachel B Slayton, Michael A Johansson, and Jay C Butler. Improving pandemic response: Employing mathematical modeling to confront coronavirus disease 2019. Clinical Infectious Diseases, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEstee Y Cramer, Evan L Ray, Velma K Lopez, Johannes Bracher, Andrea Brennen, Alvaro J Castro Rivadeneira, Aaron Gerding, Tilmann Gneiting, Katie H House, Yuxin Huang, et al.Â Evaluation of individual and ensemble probabilistic forecasts of covid-19 mortality in the us. Medrxiv, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSara Y Del Valle, Benjamin H McMahon, Jason Asher, Richard Hatchett, Joceline C Lega, Heidi E Brown, Mark E Leany, Yannis Pantazis, David J Roberts, Sean Moore, et al.Â Summary results of the 2014-2015 darpa chikungunya challenge. BMC infectious diseases, 18(1):1â14, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMichelle V Evans, Tad A Dallas, Barbara A Han, Courtney C Murdock, and John M Drake. Data-driven identification of potential zika virus vectors. elife, 6:e22053, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNikos I Bosse, Sam Abbott, Johannes Bracher, Habakuk Hain, Billy J Quilty, Mark Jit, Edwin van Leeuwen, Anne Cori, Sebastian Funk, et al.Â Comparing human and model-based forecasts of covid-19 in germany and poland. medRxiv, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDavid C Farrow, Logan C Brooks, Sangwon Hyun, Ryan J Tibshirani, Donald S Burke, and Roni Rosenfeld. A human judgment approach to epidemiological forecasting. PLoS computational biology, 13(3):e1005248, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThomas McAndrew and Nicholas G Reich. An expert judgment model to predict early stages of the covid-19 outbreak in the united states. Medrxiv, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGabriel Recchia, Alexandra LJ Freeman, and David Spiegelhalter. How well did experts and laypeople forecast the size of the covid-19 pandemic? PloS one, 16(5):e0250935, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n", 
    "url": "\/\/localhost:4321\/2022\/03\/20\/forecasting-covid-with-a-little-help-from-your-neighbors.\/"
        },
    
    
    "\/\/localhost:4321\/about\/": {
        
        "title": "About",
            "tags": [],
    "content": " Hey y'all, I'm Dave Hey ChatGPT: Take all of the data from our previous interactions and write an \"About me\" paragraph for my website that lets the good people of the Internet know what I'm about. Meet Dr. David Braunâa cognitive neuroscientist, data scientist, and experienced meditator with a knack for making complexity feel grounded. With 11+ years of research behind him, Davidâs worked at the intersection of spontaneous thought, cognitive control, decision making, and epidemiology, using tools like R, Python, EEG, and computational and predictive modeling to unpack the messiness of the human mind. Whether itâs through code, conversation, or contemplation, Davidâs all about meaningful work that helps peopleâand doing it from a quiet spot in the woods, if he can help it.\nValues include kindness, patience, thoughtfulness, humility, curiosity, balance, and simplicity.\n", 
    "url": "\/\/localhost:4321\/about\/"
        },
    
    
    "\/\/localhost:4321\/categories\/": {
        
        "title": "Categories",
            "tags": [],
    "content": "", 
    "url": "\/\/localhost:4321\/categories\/"
        },
    
    
    "\/\/localhost:4321\/publications\/": {
        
        "title": "Publications",
            "tags": [],
    "content": "Under review Braun, D., Shareef-Trudeau, L., Rao, S., Cheesebrough, C., Kam, J. W. Y., Kucyi, A. (Under review). State anxiety modulates the relationship between neural processing of heartbeats and spontaneous fluctuations in subjective arousal. Journal of Neuroscience. https://doi.org/10.1101/2025.03.26.645574 2024 McAndrew, T., Gibson, G. C., Braun, D., Srivastava, A, \u0026amp; Brown, K. (2024). Chimeric forecasting: An experiment to leverage human judgment to improve forecasts of infectious disease using simulated surveillance data. Epidemics, 47, 100756. https://doi.org/10.1016/j.epidem.2024.100756\nBounyarith, T., Braun, D., \u0026amp; Kucyi, A. (2024). Examining the neural bases of spontaneous mental experiences with real-time fMRI. Peer Community in Registered Reports [Stage 1 Registered Report: In- Principle Accepted]. https://osf.io/sd4hu\nKucyi, A., Anderson, N., Bounyarith, T., Braun, D., Shareef-Trudeau, L., Treves, I., \u0026hellip; \u0026amp; Hung, S. (2024). Individual variability in neural representations of mind wandering. Network Neuroscience, 8, 808-856. https://doi.org/10.1162/netn_a_00387.\nMcAndrew, T., Gibson, G. C., Braun, D., Srivastava, A. \u0026amp; Brown, K. (2024). Chimeric Forecasting: An experiment to leverage human judgment to improve forecasts of infectious disease using simulated surveillance data. Epidemics, 47, 100756. https://doi.org/10.1016/j.epidem.2024.100756.\nMittelstadt, V., Mackenzie, I. G., Braun, D., \u0026amp; Arrington, K. (2024). Reactive and proactive control processes in voluntary task choice. Memory and Cognition, 52, 419-429. https://doi.org/10.3758/s13421-023-01470-y\n2022 McAndrew, T., Codi, A., Cambeiro, J., Besiroglu, T., Braun, D., Chen, E., Enrique Urtubey de CÃ©saris, L., Luk, D. (2022). Chimeric forecasting: Combining probabilistic predictions from computational models and human judgment. BMC Infectious Diseases, 22, 1-17. https://doi.org/10.1186/s12879-022-07794-5.\nBraun, D., Ingram, D., Ingram, D., Khan, B., Marsh, J., \u0026amp; McAndrew, T. (2022). Crowdsourced perceptions and COVID-19: Improving computational forecasts of US national incident cases of COVID-19 with crowdsourced perceptions of human behavior. JMIR Public Health Surveill, 8, 1-18. http://dx.doi.org/10.2196/39336\nCodi, A., Luk, D., Braun, D., Cambeiro, J., Besiroglu, T. Chen, E., Enrique Urtubey de Cesaris, L., Bocchini, P., McAndrew, T. (2022). Aggregating human judgment probabilistic predictions of COVID-19 transmission, burden, and preventative measures. American Journal of Public Health. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9016644/\nMcAndrew, T., Majumder, M. S., Lover, A. A., Venkatramanan, S., Boccini, P., Besiroglu, T., Codi, A., **Braun, D.*, Dempsey, G., Abbott, S., Chevalier, S., Bosse, N. I., Cambeiro, J. (2022). Human judgment forecasts of human monkeypox transmission and burden in non-endemic countries. Lancet Digital Health. https://doi.org/10.1016/S2589-7500(22)00127-3\nEarlier Marvel, C. J., Bates, J. E., Hambric, C. E., Braun, D., Arrington, C. M., \u0026amp; Harmer, M. P. (2021). The Lehigh Presidential Nano-Human Interface Initiative: Covergence of materials and cognitive sciences. MRS Bulletin. https://doi.org/10.1557/s43577-021-00232-y\nBraun, D., Arrington, C. M. (2018). Assessing the role of effort and reward in task selection using a reward-based voluntary task switching paradigm. Psychological Research, 82, 54-64. https://doi.org/10.1007/s00426-017-0919-x\nFleck, J. I. \u0026amp; Braun, D. (2015). The impact of eye movements on a verbal creativity task. Journal of Cognitive Psychology, 27, 866-881. https://doi.org/10.1080/20445911.2015.1036057\n", 
    "url": "\/\/localhost:4321\/publications\/"
        },
    
    
    "\/\/localhost:4321\/tags\/": {
        
        "title": "Tags",
            "tags": [],
    "content": "", 
    "url": "\/\/localhost:4321\/tags\/"
        },
    
    }
</script>

<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

</footer></body>
</html>
